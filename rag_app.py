import streamlit as st

# 1. Kh·ªüi t·∫°o session state ƒë·ªÉ l∆∞u l·ªãch s·ª≠ chat n·∫øu ch∆∞a c√≥
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# 2. Hi·ªÉn th·ªã l·ªãch s·ª≠ c≈© (kh√¥ng t·ªën quota)
for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# 3. Ch·ªâ g·ªçi API khi ng∆∞·ªùi d√πng th·ª±c s·ª± nh·∫≠p li·ªáu m·ªõi
if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
    # Hi·ªÉn th·ªã c√¢u h·ªèi ng∆∞·ªùi d√πng
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.chat_history.append({"role": "user", "content": prompt})

    # G·ªçi API (L√∫c n√†y m·ªõi t·ªën 1 quota)
    try:
        response = model.generate_content(prompt)
        bot_reply = response.text
        
        # Hi·ªÉn th·ªã v√† l∆∞u c√¢u tr·∫£ l·ªùi
        with st.chat_message("assistant"):
            st.markdown(bot_reply)
        st.session_state.chat_history.append({"role": "assistant", "content": bot_reply})
        
    except Exception as e:
        st.error(f"H·∫øt l∆∞·ª£t d√πng: {e}")


import google.generativeai as genai
from pinecone import Pinecone

# 1. C·∫•u h√¨nh trang
st.set_page_config(page_title="Chatbot Nh√¢n S·ª± (RAG Demo)", layout="centered")
st.title("ü§ñ Tr·ª£ l√Ω HR th√¥ng minh")
st.caption("H·ªèi ƒë√°p d·ª±a tr√™n quy ƒë·ªãnh n·ªôi b·ªô (D·ªØ li·ªáu t·ª´ Pinecone)")

# 2. Sidebar: Nh·∫≠p Key (ƒê·ªÉ k·∫øt n·ªëi 2 ƒë·∫ßu m·ªëi)
with st.sidebar:
    st.header("üîê C·∫•u h√¨nh API")
    google_api_key = st.text_input("Google API Key:", type="password")
    pinecone_api_key = st.text_input("Pinecone API Key:", type="password")
    index_name = st.text_input("T√™n Index Pinecone:", value="demo-rag-it1994")
    
    st.info("L∆∞u √Ω: Ph·∫£i d√πng ƒë√∫ng Key v√† T√™n Index b·∫°n ƒë√£ t·∫°o tr√™n Colab.")

# 3. H√†m x·ª≠ l√Ω logic (Core Functions)
def get_embedding(text):
    """Bi·∫øn c√¢u h·ªèi th√†nh Vector (gi·ªëng h·ªát l√∫c n·∫°p d·ªØ li·ªáu)"""
    try:
        result = genai.embed_content(
            model="models/text-embedding-004",
            content=text,
            task_type="retrieval_query" # L∆∞u √Ω: type l√† query
        )
        return result['embedding']
    except Exception as e:
        st.error(f"L·ªói t·∫°o vector: {e}")
        return None

def query_pinecone(vector, index_name, api_key):
    """G·ª≠i vector l√™n Pinecone ƒë·ªÉ t√¨m ki·∫øm"""
    pc = Pinecone(api_key=api_key)
    index = pc.Index(index_name)
    
    results = index.query(
        vector=vector,
        top_k=3, # L·∫•y 3 ƒëo·∫°n vƒÉn b·∫£n li√™n quan nh·∫•t
        include_metadata=True
    )
    return results

# 4. Giao di·ªán Chat
if "messages" not in st.session_state:
    st.session_state.messages = []

# Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# X·ª≠ l√Ω khi ng∆∞·ªùi d√πng nh·∫≠p c√¢u h·ªèi
if prompt := st.chat_input("B·∫°n mu·ªën h·ªèi g√¨ v·ªÅ quy ƒë·ªãnh c√¥ng ty?"):
    # Hi·ªán c√¢u h·ªèi ng∆∞·ªùi d√πng
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Ki·ªÉm tra Key
    if not google_api_key or not pinecone_api_key:
        st.error("Vui l√≤ng nh·∫≠p ƒë·ªß API Key b√™n tay tr√°i!")
        st.stop()

    # AI X·ª≠ l√Ω
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            # B∆Ø·ªöC 1: C·∫•u h√¨nh Google
            genai.configure(api_key=google_api_key)
            
            # B∆Ø·ªöC 2: T√¨m ki·∫øm ng·ªØ c·∫£nh (Retrieval)
            message_placeholder.markdown("üîç *ƒêang tra c·ª©u t√†i li·ªáu...*")
            question_vector = get_embedding(prompt)
            
            if question_vector:
                search_results = query_pinecone(question_vector, index_name, pinecone_api_key)
                
                # T·ªïng h·ª£p th√¥ng tin t√¨m ƒë∆∞·ª£c th√†nh 1 ƒëo·∫°n vƒÉn (Context)
                context_text = ""
                for match in search_results['matches']:
                    # Ch·ªâ l·∫•y tin c√≥ ƒë·ªô tin c·∫≠y > 50%
                    if match['score'] > 0.5: 
                        context_text += f"- {match['metadata']['text_content']}\n"
                
                if not context_text:
                    context_text = "Kh√¥ng t√¨m th·∫•y th√¥ng tin c·ª• th·ªÉ trong t√†i li·ªáu."

                # B∆Ø·ªöC 3: T·∫°o Prompt cho AI (Augmented Generation)
                # ƒê√¢y l√† k·ªπ thu·∫≠t "Grounding" - √âp AI ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n Context
                final_prompt = f"""
                B·∫°n l√† tr·ª£ l√Ω nh√¢n s·ª±. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y.
                N·∫øu th√¥ng tin kh√¥ng c√≥ trong ng·ªØ c·∫£nh, h√£y n√≥i "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong quy ƒë·ªãnh".
                ƒê·ª´ng t·ª± b·ªãa ra th√¥ng tin.

                Th√¥ng tin ng·ªØ c·∫£nh (Context):
                {context_text}

                C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:
                {prompt}
                """

                # B∆Ø·ªöC 4: G·ª≠i cho Gemini tr·∫£ l·ªùi
                model = genai.GenerativeModel('gemini-2.5-flash')
                response = model.generate_content(final_prompt)
                
                full_response = response.text
                message_placeholder.markdown(full_response)
        
        except Exception as e:
            full_response = f"C√≥ l·ªói x·∫£y ra: {str(e)}"
            message_placeholder.error(full_response)
            
    # L∆∞u l·ªãch s·ª≠
    st.session_state.messages.append({"role": "assistant", "content": full_response})
